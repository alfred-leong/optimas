import litellm
import json
from transformers import AutoModelForCausalLM, AutoTokenizer

HF_MODELS = [
    "Qwen/Qwen2.5-7B-Instruct",
    "meta-llama/Llama-3.1-8B-Instruct"
]

def get_llm_output(message, 
                   model="gpt-4o", 
                   max_new_tokens=4096, 
                   temperature=1, 
                   json_object=False,
                   system_prompt=None,
                   **generation_kwargs):
    """
    Use litellm to complete a prompt using the specified model.

    Args:
        message (str or list): The input message or a list of message dicts.
        model (str): The model to use for completion.
        max_new_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature.
        json_object (bool): Whether to output in JSON format.
        system_prompt (str): Optional system prompt.

    Returns:
        str: The completed text generated by the model.
    """
    # Convert message to litellm-compatible format if needed
    if isinstance(message, str):
        messages = [{"role": "user", "content": message}]
        if system_prompt:
            messages.insert(0, {"role": "system", "content": system_prompt})
    else:
        messages = message

    kwargs = {
        "model": model,
        "messages": messages,
        "max_tokens": max_new_tokens,
        "temperature": temperature,
    }
    kwargs.update(generation_kwargs)

    if json_object:
        kwargs["response_format"] = {"type": "json_object"}

    if model in HF_MODELS:
        # Handle Hugging Face models separately
        response = get_response_from_hf_model(**kwargs)
        content = response
    else:
        # Call litellm
        response = litellm.completion(**kwargs)

        # litellm returns an object similar to OpenAI
        content = response.choices[0].message.content
    if json_object:
        return json.loads(content)
    else:
        return content

def get_response_from_hf_model(model, messages, max_tokens, **kwargs):
    model_id = model
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype="auto", device_map="auto")
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer([text], return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=max_tokens, **kwargs)
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(response_text)
    return response_text